{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TFLite 변환\n",
    "주의점: 모든 TF ops가 변환 가능한 것은 아니다. 그럴 때 변환된 모델의 크기를 좀 더 늘려서라도 지원되는\n",
    "TF ops를 늘리고 싶거나, 직접 정의한 ops를 추가하고 싶다면:\n",
    "```Python\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "converter.target_spec.experimental_select_user_tf_ops = [\n",
    "    'your_op_name1',\n",
    "    'your_op_name2'\n",
    "]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 03:17:49.657647: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 03:19:21.650291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 변환기 빌드 및 옵션 설정\n",
    "saved_model_dir = \"models/2\"\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "# 사전 제공 옵션: DEFAULT(지연과 크기 사이 균형), OPTIMIZE_FOR_LATENCY, OPTIMIZE_FOR_SIZE\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 제공한 옵션으로 모델 변환\n",
    "tflite_model = converter.convert()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 파일로 모델 기록\n",
    "lite_model_dir = \"models/lite_model\"\n",
    "with open(lite_model_dir, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFLite 모델 serving\n",
    "`docker run --use_tflite_model=true` 옵션만 추가하면 가능."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prometheus로 모델 서버 모니터링\n",
    "Serving과 Prometheus 컨테이너를 연다. 프로메테우스가 외부에서 지표 수집 요청을 받으면 프로메테우스는\n",
    "모델 서버가 공개한 엔드포인트에서 데이터를 받아온다.\n",
    "\n",
    "1. 먼저 /tmp/prometheus.yml에 구성 정보를 설정한다.\n",
    "```\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "  evaluation_interval: 15s\n",
    "  external_labels:\n",
    "    monitor: 'tf-serving-monitor'\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'prometheus'\n",
    "    scrape_interval: 5s\n",
    "    # 아래 경로는 serving에겐 지표를 저장, prometheus에겐 지표 수집 경로이다.\n",
    "    metrics_path: /monitoring/prometheus/metrics\n",
    "    static_configs:\n",
    "    # REST 요청만 가능. 도커 도메인 이름 확인 기능 덕분에 아래 호스트명을 쓰면 8501로 공개된 주소를\n",
    "    # 자동으로 찾을 수 있다.\n",
    "    - targets: ['host.docker.internal:8501']\n",
    "```\n",
    "\n",
    "2. 프로메테우스 컨테이너 가동\n",
    "```bash\n",
    "docker run -p 9090:90090 \\\n",
    "           -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \\\n",
    "           -t prom/prometheus\n",
    "```\n",
    "\n",
    "3. 서빙 설정\n",
    "```\n",
    "prometheus_config {\n",
    "  enable: true,\n",
    "  # 아래 경로는, prometheus.yml 속 metrics_path와 동일해야 한다.\n",
    "  path: \"/monitoring/prometheus/metrics\"\n",
    "}\n",
    "```\n",
    "\n",
    "4. 서빙 컨테이너 기동\n",
    "```bash\n",
    "docker run -p 8501:8501 \\\n",
    "           # source: path to the file or directory on the Docker daemon host.\n",
    "           # target: the path where the file or directory is mounted in the container.\n",
    "           --mount type=bind,source=<model_path>,target=<model_path> \\\n",
    "           --mount type=bind,source=<config_path>,target=<config_path> \\\n",
    "           --monitoring_config_file=<config_in_docker_filessystem> \\\n",
    "           -t tensorflow/serving\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
